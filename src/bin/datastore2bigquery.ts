#!/usr/bin/env node
/*
 * datastore2bigquery.ts
 *
 * Created by Dr. Maximillian Dornseif 2021-12-23 in datastore-to-bigquery 1.0.2
 * Copyright (c) 2021 Dr. Maximillian Dornseif
 */

import { BigQuery } from '@google-cloud/bigquery';
import { Datastore } from '@google-cloud/datastore';
import { Storage } from '@google-cloud/storage';
import { ArgumentParser } from 'argparse';
import { dumpAllKinds } from 'datastore-backup';
import ora from 'ora';

import {
  ensureDataset,
  loadAllKindsFromPrefix,
} from '../lib/load-into-to-bigquery';

const parser = new ArgumentParser({
  description: 'Copy datastore Contents to BigQuery.',
  epilog:
    'Please provide `GOOGLE_APPLICATION_CREDENTIALS` via the Environment!',
  add_help: true,
});

parser.add_argument('projectId', { help: 'Datastore project ID' });
parser.add_argument('-b', '--bucket', {
  help: 'GCS bucket to store backup. Needs to be in the same Region as datastore. (default: projectId.appspot.com',
});
parser.add_argument('-d', '--backupDir', {
  default: 'bak',
  help: 'prefix/dir within bucket',
});
parser.add_argument('-n', '--backupName', {
  help: 'name of backup (default: autogenerated)',
});
parser.add_argument('-s', '--namespace', { help: 'datastore namespace' });

parser.add_argument('-p', '--bqProjectId', {
  help: 'BigQuery project ID. (default: same as datastore)',
});
parser.add_argument('--datasetName', {
  help: 'Name of BigQuery Dataset to write to. Needs to be in the same Region as GCS bucket. (default: same as projectId)',
});

const args = parser.parse_args();

async function main() {
  const bucket = args.bucket || `${args.projectId}.appspot.com`;
  const bqProjectId = args.bqProjectId || args.projectId;
  const datasetName = args.datasetName || args.projectId;
  const spinner = ora().start('ðŸŒˆ Unicorns! âœ¨ðŸŒˆ');

  try {
    const datastore = new Datastore({
      projectId: args.projectId,
      namespace: args.namespace,
    });
    // how to get datastore location?
    //https://stackoverflow.com/questions/70461019

    const bigquery = new BigQuery({ projectId: bqProjectId });

    spinner.text = `getting location for bucket ${bucket}`;
    const storage = new Storage();
    const [meta] = await storage.bucket(bucket).getMetadata();
    spinner.info(`bucket ${bucket} is in ${meta.location}`);

    spinner.text = 'getting dataset';
    // create Dataset if needed.
    const dataset = await ensureDataset(
      bigquery,
      datasetName,
      spinner,
      meta.location
    );
    spinner
      .info(
        `dataset ${args.projectId}:${datasetName} is in ${
          dataset.location || 'unknown location'
        }`
      )
      .start();
    if (dataset.location && dataset.location != meta.location) {
      spinner
        .warn(
          `Dataset and bucket locations do not match. This might load to difficulties.`
        )
        .start();
    }

    const backupUrls = await dumpAllKinds(
      datastore,
      bucket,
      args.backupName,
      args.backupDir,
      spinner
    );
    const backupPaths = backupUrls.map((x) =>
      x.split('/').slice(3, -1).join('/').replace(/-\d+$/, '')
    );

    await loadAllKindsFromPrefix(
      bigquery,
      datasetName,
      bucket,
      backupPaths.pop(),
      spinner
    );
  } catch (error) {
    spinner.warn(error.message);
    throw error;
  }

  return '';
}

main().then(console.log).catch(console.error);
